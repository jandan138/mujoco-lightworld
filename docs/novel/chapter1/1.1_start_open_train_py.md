# 第 1 章 初见世界 — 1.1 起点：打开 train.py

你是这部故事的旁白，也是第一位读者。
今天，你按下电源键，屏幕亮起，VSCode 打开，然后你双击一个文件：`train.py`。

你看到了一段机器的旅程。它还没有名字，但它即将拥有意志。

---

## 你将获得什么
- 读懂 `train.py` 作为“总导演”的角色：它如何把环境、策略、价值、记忆、世界模型与训练循环串起来。
- 找到世界模型的“挂载点”，理解我们是如何在不打乱 PPO 的节奏下让它协同工作。
- 能够运行一个最小实验，验证从零到动作、从动作到奖励的整条链条在你电脑上真实发生。

## 场景一：剧本源头（配置与命令行）
在任何故事开始前，都会有“剧本”。我们的剧本在 `config.yaml`，同时你也能用命令行覆盖它的部分台词。

- 关键字段：
  - `experiment.task`：选择环境，例如 `Walker2d-v4`。
  - `ppo.*`：学习率、剪切比、目标 KL 等。
  - `wm.*`：是否启用世界模型、是否启用辅助奖励、潜空间维度 `z_dim`、权重 `alpha`。
- 你可以这样开场：
  - `python train.py --task Walker2d-v4 --use_wm True --z_dim 32 --alpha 0.01`

提示：具体参数说明见 `docs/config/Parameters.md`。

## 场景二：舞台搭好（环境与随机种子）
`train.py` 会创建 Gymnasium + MuJoCo 的环境（比如 Walker2d）。
这是你的舞台，智能体将在这里学习如何“走路”。
- 设置随机种子让每次开场尽可能一致；
- 创建环境后拿到 `obs`，这就是第一幕的第一句台词：世界对你说“我现在是什么样”。

若遇到安装问题，先翻 `docs/basics/MuJoCo_Setup.md` 与 `docs/pitfalls/CommonIssues.md`。

## 场景三：主角登场（Actor 与 Critic）
在 `rl/policy.py`，你会遇到两个角色：
- `Actor`：根据输入（可以是原始观察，也可以是经过 Encoder 的潜特征 `z`）产生动作分布，然后采样动作；
- `Critic`：评估状态价值，用于计算优势与更新策略的尺度。

它们被 `PPOTrainer` 组织成一个团队，具体训练节奏在 `rl/ppo.py`。

## 场景四：旅途的记忆（Buffer 与 GAE）
每一次与环境的互动（`obs, act, rew, next_obs, done`）都被存进 `RolloutBuffer`。
当一个“路径”结束，`finish_path` 会用 GAE-Lambda（见 `docs/basics/RL_Basics.md`）计算优势与回报，这些数字会成为下一次更新的燃料。

## 场景五：影子的地图（世界模型挂载点）
这一幕很关键，因为它是“小说的暗线”：
- 在 `train.py` 中，有一行把世界模型的损失函数接入：
  - `"loss_fn": world_model_loss,`（你可以在编辑器中搜索这行）
- 启用 `wm.use_wm` 时，`Encoder` 会把 `obs` 投影到潜空间 `z`；
- 启用 `wm.use_aux_reward` 时，`prediction_error_reward` 会把 Dynamics 的预测误差转成“奖励加成”（权重由 `alpha` 控制）。

这意味着：
- 我们既能用更“干净”的特征输入 PPO（特征模式），也能把预测误差作为探索激励（奖励模式）；
- 但我们不改变 PPO 的核心节奏，只是让世界模型成为一个“可靠的影子”。

更多细节见 `docs/basics/PPO_WorldModel_Concepts.md` 与 `wm/loss.py`。

## 场景六：训练循环（从采样到更新）
现在，镜头拉到训练主循环：
- 采样：用 `Actor` 选择动作，环境返回奖励与下一观察；
- 存储：`buffer.store(...)` 收集轨迹；
- 结算：每条路径结束时计算优势与回报；
- 更新：`PPOTrainer.update(...)` 执行多次迭代的策略与价值更新，使用剪切与 KL 控制保持稳定；
- （若启用）世界模型在线训练：依据 `loss_fn` 更新 Encoder/Dynamics；
- 日志与保存：把训练曲线写入 `results/logs/`，定期保存模型到 `results/models/`。

这就是一整天的旅程。

---

## 剧情推进（你现在就能做的事）
1) 运行最小实验（CPU 上也能跑）：
- `python train.py --task Walker2d-v4 --device cpu --total_steps 5000 --use_wm False`
- 观察 `results/logs/` 是否生成 CSV；用 `docs/runbook/Plotting.md` 绘制第一条学习曲线。

2) 再加上世界模型的影子：
- `python train.py --task Walker2d-v4 --device cpu --total_steps 5000 --use_wm True --use_aux_reward True --z_dim 32 --alpha 0.01`
- 对比两次运行的曲线，看看影子是否带来更快的起步或更稳的上升。

3) 在编辑器里搜索“世界模型挂载点”：
- 定位 `"loss_fn": world_model_loss,` 这一行；
- 打开 `wm/loss.py`，阅读 `world_model_loss` 的输入输出。

## 小练习（5 分钟）
- 问题 A：为什么我们要设置 `target_kl` 和 `clip_ratio`？（提示：它们是 PPO 稳定更新的“护栏”。）
- 问题 B：当 `alpha` 过大时，会发生什么？（提示：辅助奖励可能压过环境奖励，影响策略方向。）
- 问题 C：`z_dim` 太小或太大分别会有什么影响？（提示：表达不足 vs 过拟合。）

答案线索分别在：`docs/basics/RL_Basics.md`、`docs/basics/PPO_WorldModel_Concepts.md`、`docs/performance/Tips.md`。

## 术语卡片
- 观察 `obs`：环境当前状态的数值表示。
- 动作 `act`：智能体对环境的回应（例如关节力矩）。
- 奖励 `rew`：环境对动作的评价（越大越好）。
- 价值 `V(s)`：从当前状态出发能得到的长期回报的估计。
- 优势 `A(s,a)`：动作相对基线（价值）的“超额收益”。

## 小结与悬念
你已经完成了第一眼的巡礼：
- 知道 `train.py` 如何组织整部剧；
- 找到了世界模型在剧中的位置；
- 跑通了最小实验并看到了第一条曲线。

下一节 1.2，我们将正式介绍舞台：Gymnasium + MuJoCo，到底是什么让“走路”这件事在虚拟世界里拥有真实的挑战。

— 继续翻页吧。