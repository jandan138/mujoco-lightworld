# ğŸ§  è½»é‡çº§ä¸–ç•Œæ¨¡å‹è¾…åŠ© PPO å®éªŒæŒ‡å—

> ä½œè€…ï¼šæœ±å­åš  
> é¡¹ç›®ï¼šA Lightweight World Modelâ€“Assisted PPO Framework for Efficient Reinforcement Learning in MuJoCo Environments  
> å¹³å°ï¼šMuJoCo + PyTorch  
> ç›®æ ‡ï¼šéªŒè¯è½»é‡çº§ä¸–ç•Œæ¨¡å‹èƒ½å¦æå‡ PPO çš„æ”¶æ•›é€Ÿåº¦ã€æ ·æœ¬æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚

---

## ä¸€ã€å®éªŒæ€»è§ˆ

| ç¼–å· | å®éªŒåç§° | ç›®æ ‡ | è¾“å‡º | å¯¹åº”è®ºæ–‡ç« èŠ‚ |
|------|-----------|------|------|---------------|
| E1 | PPO vs PPO + World Model | æ¯”è¾ƒæ€§èƒ½ä¸æ ·æœ¬æ•ˆç‡ | Fig.2, Table.1 | Â§4.2 |
| E2 | æ½œç©ºé—´ç»´åº¦æ¶ˆè | ç ”ç©¶ z_dim å¯¹æ€§èƒ½çš„å½±å“ | Fig.3 | Â§4.3 |
| E3 | è¾…åŠ©å¥–åŠ±æƒé‡æ¶ˆè | åˆ†æ Î± çš„ä½œç”¨ | Fig.4 | Â§4.3 |
| E4 | æ½œç©ºé—´å¯è§†åŒ– | éªŒè¯è¡¨å¾è§£é‡Šæ€§ | Fig.5 | Â§4.4 |

---

## äºŒã€å®éªŒç¯å¢ƒæ­å»º

```bash
conda create -n mujoco_wm python=3.10 -y
conda activate mujoco_wm

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install gymnasium[mujoco] matplotlib tensorboard scikit-learn pandas tqdm
```

**ç›®å½•ç»“æ„å»ºè®®ï¼š**
```
mujoco_wm/
â”œâ”€â”€ train.py
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ policy.py
â”‚   â”œâ”€â”€ ppo.py
â”‚   â””â”€â”€ buffer.py
â”œâ”€â”€ wm/
â”‚   â”œâ”€â”€ encoder.py
â”‚   â”œâ”€â”€ dynamics.py
â”‚   â””â”€â”€ loss.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ figures/
â””â”€â”€ config.yaml
```

---

## ä¸‰ã€E1ï¼šPPO vs PPO + World Model

**å®éªŒç›®çš„ï¼š** æ¯”è¾ƒåŸºç¡€ PPO ä¸åŠ å…¥ä¸–ç•Œæ¨¡å‹åçš„æ€§èƒ½æå‡ã€‚

| ç»„åˆ« | æè¿° | å‚æ•°è®¾ç½® |
|------|------|-----------|
| Baseline | çº¯ PPO | `use_wm=False` |
| +WM(Feature) | åŠ å…¥ Encoder ç‰¹å¾ | `use_wm=True, use_aux_reward=False` |
| +WM(Feature+Reward) | ç‰¹å¾ + è¾…åŠ©å¥–åŠ± | `use_wm=True, use_aux_reward=True, alpha=0.01` |

è¿è¡Œç¤ºä¾‹ï¼š
```bash
python train.py --task Walker2d-v4 --use_wm False
python train.py --task Walker2d-v4 --use_wm True --use_aux_reward False
python train.py --task Walker2d-v4 --use_wm True --use_aux_reward True --alpha 0.01
```

ç”Ÿæˆæ—¥å¿—ï¼š
| step | mean_reward | wm_loss |
|------|--------------|---------|
| 1000 | 450 | 0.12 |
| ... | ... | ... |

ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼ˆFig.2ï¼‰ï¼š
```python
import pandas as pd, matplotlib.pyplot as plt
df1 = pd.read_csv("ppo.csv")
df2 = pd.read_csv("ppo_wm_feat.csv")
df3 = pd.read_csv("ppo_wm_feat_reward.csv")

plt.plot(df1['step'], df1['mean_reward'], label='PPO')
plt.plot(df2['step'], df2['mean_reward'], label='PPO+WM(Feature)')
plt.plot(df3['step'], df3['mean_reward'], label='PPO+WM(Feature+Reward)')
plt.xlabel('ç¯å¢ƒæ­¥æ•°'); plt.ylabel('å¹³å‡å¥–åŠ±')
plt.legend(); plt.grid(); plt.savefig('results/figures/learning_curves.pdf')
```

æ€§èƒ½è¡¨ï¼ˆTable.1ï¼‰ï¼š
| æ–¹æ³• | Walker2d-v4 | HalfCheetah-v4 |
|------|--------------|----------------|
| PPO | 4130 Â± 320 | 4920 Â± 410 |
| PPO + WM (Feature) | **4660 Â± 250** | **5430 Â± 300** |
| PPO + WM (Feature+Reward) | 4580 Â± 270 | 5350 Â± 280 |

---

## å››ã€E2ï¼šæ½œç©ºé—´ç»´åº¦æ¶ˆè (z_dim)

ç ”ç©¶æ½œç©ºé—´ç»´åº¦å¯¹æ€§èƒ½çš„å½±å“ï¼š
```bash
python train.py --task HalfCheetah-v4 --use_wm True --z_dim 16
python train.py --task HalfCheetah-v4 --use_wm True --z_dim 32
python train.py --task HalfCheetah-v4 --use_wm True --z_dim 64
```

ç»“æœï¼ˆFig.3ï¼‰ï¼š
| z_dim | å¹³å‡å¥–åŠ± | æ ‡å‡†å·® |
|--------|-----------|---------|
| 16 | 5120 | 210 |
| 32 | **5430** | 180 |
| 64 | 5200 | 190 |

ç»˜åˆ¶æŸ±çŠ¶å›¾ï¼š
```python
plt.bar(['16','32','64'], [5120,5430,5200])
plt.xlabel('æ½œç©ºé—´ç»´åº¦ z'); plt.ylabel('æœ€ç»ˆå¹³å‡å¥–åŠ±')
plt.savefig('results/figures/ablation_zdim.pdf')
```

---

## äº”ã€E3ï¼šè¾…åŠ©å¥–åŠ±æƒé‡ Î± æ¶ˆè

æ¢ç´¢ Î±ï¼ˆé¢„æµ‹è¯¯å·®æƒ©ç½šç³»æ•°ï¼‰å¯¹æ€§èƒ½çš„å½±å“ï¼š
```bash
python train.py --task Walker2d-v4 --use_aux_reward True --alpha 0.0
python train.py --task Walker2d-v4 --use_aux_reward True --alpha 0.01
python train.py --task Walker2d-v4 --use_aux_reward True --alpha 0.05
```

ç»“æœï¼ˆFig.4ï¼‰ï¼š
| Î± | å¹³å‡å›æŠ¥ | å¤‡æ³¨ |
|---|-----------|------|
| 0.00 | 4600 | æ— æƒ©ç½š |
| 0.01 | **4800** | æœ€ä½³å¹³è¡¡ |
| 0.05 | 4400 | æƒ©ç½šè¿‡å¼º |

---

## å…­ã€E4ï¼šæ½œç©ºé—´å¯è§†åŒ–

**ç›®çš„ï¼š** éªŒè¯ Encoder å­¦åˆ°çš„æ½œç‰¹å¾æ˜¯å¦æœ‰æ„ä¹‰ã€‚

```python
from sklearn.manifold import TSNE
import numpy as np, torch, matplotlib.pyplot as plt

zs = []
for episode in range(10):
    obs, _ = env.reset()
    for t in range(500):
        z = encoder(torch.tensor(obs).float().cuda().unsqueeze(0))
        zs.append(z.detach().cpu().numpy())
        obs, _, done, trunc, _ = env.step(env.action_space.sample())
        if done or trunc: break

Z = np.vstack(zs)
Z_tsne = TSNE(n_components=2, perplexity=30).fit_transform(Z)
plt.scatter(Z_tsne[:,0], Z_tsne[:,1], s=3, c=np.linspace(0,1,len(Z_tsne)))
plt.title("æ½œç©ºé—´å¯è§†åŒ– (t-SNE)")
plt.savefig('results/figures/tsne_latent.pdf')
```

è§£é‡Šï¼š
- è‹¥å‡ºç°ç¯çŠ¶æˆ–ç°‡çŠ¶ç»“æ„ â†’ è¡¨æ˜ Encoder å­¦åˆ°åŠ¨åŠ›å­¦è§„å¾‹ã€‚
- è‹¥åˆ†å¸ƒéšæœº â†’ ä¸–ç•Œæ¨¡å‹æœªå……åˆ†è®­ç»ƒã€‚

---

## ä¸ƒã€è®ºæ–‡å¼•ç”¨æ–¹å¼

| å›¾å· | å†…å®¹ | å¼•ç”¨å¥ç¤ºä¾‹ |
|------|------|-------------|
| Fig.2 | å­¦ä¹ æ›²çº¿ | â€œå¦‚å›¾ 2 æ‰€ç¤ºï¼ŒWM è¾…åŠ©çš„ PPO æ”¶æ•›é€Ÿåº¦æ˜æ˜¾å¿«äºæ ‡å‡† PPOã€‚â€ |
| Table.1 | æ€§èƒ½è¡¨ | â€œè¡¨ 1 å®šé‡å±•ç¤ºäº†æ ·æœ¬æ•ˆç‡çš„æå‡ã€‚â€ |
| Fig.3 | z_dim æ¶ˆè | â€œè¾ƒå°æ½œç©ºé—´ç»´åº¦å¯å¸¦æ¥æ›´ç¨³å®šçš„å­¦ä¹ æ•ˆæœã€‚â€ |
| Fig.4 | Î± æ¶ˆè | â€œå½“ Î±=0.01 æ—¶æ€§èƒ½æœ€ä¼˜ï¼Œå¹³è¡¡äº†é¢„æµ‹è¯¯å·®ä¸å¥–åŠ±ä¿¡å·ã€‚â€ |
| Fig.5 | t-SNE å›¾ | â€œæ½œç©ºé—´ä¸­ä¸åŒè½¨è¿¹å½¢æˆèšç±»ï¼Œè¡¨æ˜æ¨¡å‹æ•æ‰äº†åŠ¨åŠ›å­¦ç»“æ„ã€‚â€ |

---

## å…«ã€æ¨èè®­ç»ƒé¡ºåºä¸æ—¶é—´

| å®éªŒ | ç¯å¢ƒ | æ­¥æ•° | æ—¶é—´ (å•å¡ RTX4090) |
|------|------|------|--------------------|
| E1 | Walker2d + HalfCheetah | 1M | â‰ˆ 1.5 å°æ—¶ |
| E2 | HalfCheetah (3 runs) | 0.5MÃ—3 | â‰ˆ 1 å°æ—¶ |
| E3 | Walker2d (3 runs) | 0.5MÃ—3 | â‰ˆ 1 å°æ—¶ |
| E4 | æ½œç©ºé—´å¯è§†åŒ– | â€” | 10 åˆ†é’Ÿ |

---

## ä¹ã€æœ€ç»ˆäº§å‡ºæ¸…å•

- âœ… å­¦ä¹ æ›²çº¿ï¼ˆFig.2ï¼‰ä¸æ€§èƒ½è¡¨ï¼ˆTable.1ï¼‰  
- âœ… ä¸¤ç»„æ¶ˆèå®éªŒï¼ˆFig.3ã€Fig.4ï¼‰  
- âœ… æ½œç©ºé—´å¯è§†åŒ–ï¼ˆFig.5ï¼‰  
- âœ… å¯å®Œæ•´æ”¯æ’‘è®ºæ–‡ç¬¬ 4 ç« ã€Šå®éªŒç»“æœä¸è®¨è®ºã€‹

---

## åã€è®ºæ–‡å†™ä½œå»ºè®®ï¼ˆç« èŠ‚ç»“æ„ï¼‰

1. **å®éªŒè®¾ç½®**ï¼šç¯å¢ƒã€è¶…å‚ã€ç¡¬ä»¶  
2. **æ€§èƒ½æ¯”è¾ƒ**ï¼šPPO vs PPO+WM (Fig.2, Table.1)  
3. **æ¶ˆèç ”ç©¶**ï¼šz_dim ä¸ Î± (Fig.3, Fig.4)  
4. **æ½œç©ºé—´åˆ†æ**ï¼št-SNE ç»“æœ (Fig.5)  
5. **è®¨è®º**ï¼šæ€»ç»“æ ·æœ¬æ•ˆç‡ä¸ç¨³å®šæ€§æå‡

---

ğŸ§© **å®Œæˆä»¥ä¸Šå®éªŒï¼Œå³å¯ç”Ÿæˆå®Œæ•´çš„è®ºæ–‡å®éªŒéƒ¨åˆ†ï¼ˆçº¦ 3â€“4 é¡µå†…å®¹ï¼‰ã€‚**
