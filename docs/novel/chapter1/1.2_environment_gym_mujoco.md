# 第 1 章 初见世界 — 1.2 环境登场：Gymnasium + MuJoCo 是什么？

灯光亮起，你看到一片广阔的舞台。它的名字叫“环境”。
智能体在这里出生、试错、成长；每一步都是真实的物理互动。
这章，我们用最少的术语、最多的直觉，把舞台讲清楚。

---

## 你将获得什么
- 认识 Gymnasium 的基本交互方式：`reset` 与 `step` 的输入输出。
- 理解 MuJoCo 连续控制任务的“动作空间”和“观察空间”。
- 分清 `terminated` 与 `truncated` 的区别，并知道如何组合成 `done`。
- 知道在 `train.py` 里环境是如何被创建、播种（随机种子）和使用的。

## 场景一：舞台的尺寸（观察与动作空间）
环境就像一个剧场，它的“台面尺寸”由两样东西定义：
- 观察空间 `observation_space`：给智能体看的世界状态（例如关节角度、速度等）。
- 动作空间 `action_space`：智能体能做出的动作（例如施加在关节上的力矩）。

你可以在 Python 里用几行代码“量一量舞台”：

```python
import gymnasium as gym
env = gym.make("Walker2d-v4")
obs, info = env.reset(seed=42)
print("obs shape:", env.observation_space.shape)
print("act shape:", env.action_space.shape)
print("act low/high:", env.action_space.low[:3], env.action_space.high[:3])
```

提示：
- 多数 MuJoCo 连续动作空间是 `Box(low=-1, high=1, shape=(n,))`，但请以实际 `env.action_space` 为准。
- 模型的 `Actor` 会输出一个动作向量，你需要保证它不违背动作空间的边界（超界会被裁剪或导致不稳定）。

【补充解读：什么是 `obs`、`info`？这些打印会是什么样？“智能体”是谁？】
- `obs`（observation，观察）：
  - 本质是一个 `numpy.ndarray` 浮点向量，表示此刻环境的状态（如关节角度、角速度、躯干位置等）。
  - 对于 `Walker2d-v4`，典型形状为 `(17,)`（不同环境会不同）。
  - 示例（仅示意）：`obs[:5] = [0.01, -0.03, 1.02, 0.05, -0.02]`。
- `info`（额外信息）：
  - 一个字典 `dict`，包含一些调试或诊断的附加字段。多数训练流程中可以忽略。
  - 示例：`{'time': 0, 'reward_run': 0.0, ...}`（实际键会因环境而异）。
- 打印可能的输出（以 `Walker2d-v4` 为例，实际请以你电脑上的输出为准）：
  - `obs shape: (17,)`
  - `act shape: (6,)`
  - `act low/high: [-1. -1. -1.] [1. 1. 1.]`
  - 解释：这意味着该环境的状态向量有 17 个数；动作是长度为 6 的向量；每个动作分量的合法范围通常在 `[-1, 1]` 之间。
- “智能体”（agent）：
  - 并不是某个实体机器人，而是“一个会根据观察选择动作的程序”。
  - 在本项目里，智能体由 `Actor`（策略网络）与 `Critic`（价值网络）共同组成：`Actor` 决定做什么，`Critic` 评估状态好不好。

如果你想更直观地看 `obs` 的类型与数值，可以再加几行：

```python
print(type(obs))          # 例如：<class 'numpy.ndarray'>
print(obs[:8])            # 打印前 8 个数，感受量级与分布
print(type(info), info)   # 例如：<class 'dict'> {...}
```

## 场景二：第一句台词（reset）
任何一场演出都从布景开始：
- `obs, info = env.reset(seed=seed)`
  - `obs`：世界的初始状态（例如躯干位置、关节角速度等）。
  - `info`：额外信息（通常可忽略）。
- `seed`：随机种子，帮助你复现实验。

在 `train.py` 里，你会看到设置随机种子与创建环境的逻辑，确保每次启动的“世界”尽可能一致。

## 场景三：第二句台词（step）
演出开始，智能体作一个动作：
- `next_obs, reward, terminated, truncated, info = env.step(action)`
  - `action`：一个浮点向量，长度与 `env.action_space.shape[0]` 一致。
  - `reward`：这一步的评分（越大越好）。
  - `terminated`：是否达到“任务终止条件”（例如摔倒）。
  - `truncated`：是否达到“时间上限”（例如步数用尽）。
  - `next_obs`：下一刻的世界状态。

通常我们会定义：
```python
done = terminated or truncated
```
在 PPO 的采样循环里，`done` 用来判断路径是否结束，随后触发 GAE 结算（详见 `docs/basics/RL_Basics.md`）。

## 场景四：MuJoCo 的真实感
MuJoCo 是一个物理引擎，意味着：
- 动作是连续的、带边界的，通常代表力或力矩；
- 小的动作差异会带来真实的轨迹变化（例如步态、稳定性）；
- 奖励函数常常鼓励“走得远、站得稳”，并惩罚摔倒或不合理姿态。

这份真实感让训练更具挑战，也更有意义：
- 你会看到学习曲线不是直线，而是充满探索与收敛的波动；
- 合理的动作范围与数值稳定是成功的关键（在 `rl/policy.py` 的 `Actor` 中，我们保证输出与环境期望的形状匹配，并在训练中通过剪切/目标 KL 控制更新幅度，避免失控）。

## 场景五：在 train.py 里的用法
把镜头拉回 `train.py`：
- 创建环境（例如 `gym.make(task)`），拿到 `obs`；
- 用 `Actor` 产生动作，与环境交互得到 `next_obs, reward, terminated, truncated`；
- 用 `buffer.store(...)` 保存这一步的经历；
- 当 `done`（路径结束）时，调用 `buffer.finish_path(...)` 做 GAE 结算；
- 定期调用 `PPOTrainer.update(...)` 做策略与价值的多次迭代更新。

如果启用世界模型：
- `Encoder` 会把 `obs` 投影到潜空间 `z`；
- `Dynamics` 会尝试预测下一个 `z`，预测误差可由 `prediction_error_reward` 转为辅助奖励；
- 在 `train.py` 中你能找到如下挂载点：`"loss_fn": world_model_loss,`。

## 场景六：渲染与调试（只在早期用）
想“看见”智能体走路？可以用 `render_mode="human"` 创建环境：
```python
env = gym.make("Walker2d-v4", render_mode="human")
```
但注意：渲染会显著降低训练速度，建议只在早期调试使用。更多安装与渲染问题见 `docs/basics/MuJoCo_Setup.md` 与 `docs/pitfalls/CommonIssues.md`。

---

## 剧情推进（你现在就能做的事）
1) 环境体检：
```python
import gymnasium as gym
env = gym.make("Walker2d-v4")
obs, info = env.reset(seed=0)
print("obs shape:", env.observation_space.shape)
print("act shape:", env.action_space.shape)
for _ in range(5):
    import numpy as np
    action = np.clip(np.random.randn(*env.action_space.shape), env.action_space.low, env.action_space.high)
    next_obs, reward, terminated, truncated, info = env.step(action)
    print(reward, terminated, truncated)
```
确认你能执行 `reset` 与 `step`，并认识返回值的含义。

2) 跑一个极小训练（CPU 即可）：
- `python train.py --task Walker2d-v4 --device cpu --total_steps 3000`
- 用 `docs/runbook/Plotting.md` 画一次曲线，看奖励是否逐步升高。

3) 检查动作范围：
- 打印 `env.action_space.low[:3]` 与 `high[:3]`，确保策略输出在这个范围之内（必要时对输出进行 `clip`）。

## 小练习（5 分钟）
- 问题 A：为什么要区分 `terminated` 与 `truncated`？（提示：前者是任务失败或成功结束，后者是时间到；统计学习曲线时应分别记录。）
- 问题 B：如果策略经常输出超出动作边界的值，会发生什么？（提示：裁剪导致梯度信号失真、训练不稳定。）
- 问题 C：为什么设定随机种子很重要？（提示：复现实验与对比不同配置的关键。）

答案线索在：`docs/pitfalls/CommonIssues.md`、`docs/performance/Tips.md`、`docs/basics/MuJoCo_Setup.md`。

## 术语卡片
- `observation_space`：环境能提供的状态的形状与范围。
- `action_space`：智能体能采取的动作的形状与范围。
- `terminated`：任务终止（成功或失败）导致的结束。
- `truncated`：达到时间上限导致的结束。
- `seed`：随机种子，确保可重复性。

## 小结与悬念
你现在知道“舞台”是什么、如何与它交谈。
下一节 1.3，我们会认识主角的心智：`Actor` 如何产生动作，`Critic` 如何评估价值，它们如何配合把“走路”变成一种可以学习的能力。

— 继续翻页吧。